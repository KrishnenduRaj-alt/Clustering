# -*- coding: utf-8 -*-
"""KMeansplusplus.py.

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kmezAHqHDJURX6SeCpJDXucjgQoS7T6M
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt

def load_dataset(file_path="dataset"):
    """
   Loads the dataset from a given file. The first column is assumed to be labels,
    and the rest are the feature columns.

    Args include the path to the dataset file (default is "dataset") [file_path (str)].

    Returns a NumPy array containing the numerical values of the dataset (excluding the labels).

    Raises:
        Exception: If there is an error reading the dataset.
    """
    try:
        df = pd.read_csv(file_path, header=None, delim_whitespace=True)
        df_numeric = df.iloc[:, 1:].astype(float)  # Exclude the first column (labels)
        return df_numeric.to_numpy()
    except Exception as e:
        print(f"Error loading dataset: {e}")
        exit(1)

def compute_distance(point1, point2):
    """
    Computes the Euclidean distance between given data points.

    Args include:
        1. The first data point [point 1].
        2. The second data point [point 2].

    Returns the Euclidean distance between the two points (float).
    """
    return np.linalg.norm(point1 - point2)

def kmeans_plus_plus_init(data, k):
    """
    Initialisation of k cluster centers using the K-Means++ algorithm to spread out initial centers.

    Args include:
        1. The dataset from which to initialize cluster centers [data (np.ndarray)].
        2. The number of clusters [ k (int)].

    Returns a NumPy array containing the initial k cluster centers
    """
    np.random.seed(42)
    centers = [data[np.random.choice(len(data))]]
    for _ in range(1, k):
        distances = np.array([min([compute_distance(x, c) for c in centers]) for x in data])
        probabilities = distances ** 2 / np.sum(distances ** 2)
        new_center = data[np.random.choice(len(data), p=probabilities)]
        centers.append(new_center)
    return np.array(centers)

def assign_cluster_ids(data, centers):
    """
    Assigns each data point to the nearest cluster center.

    Args include:
        1. The dataset to be clustered [data (np.ndarray)].
        2. The current cluster centers [centers


    Returns an array of cluster IDs, one for each data point.
    """
    return np.array([np.argmin([compute_distance(point, center) for center in centers]) for point in data])

def compute_cluster_representatives(data, cluster_ids, k):
    """
    Computes the new cluster centers as the mean of all data points assigned to each cluster.
    If a cluster has no points, it reinitializes the center randomly.

    Args:
        1. The dataset to be clustered [data (np.ndarray)].
        2. The current cluster [cluster_ids (np.ndarray)]
    """
    new_centers = np.zeros((k, data.shape[1]))
    cluster_ids = np.array(cluster_ids)
    for i in range(k):
        cluster_points = data[cluster_ids == i]
        if cluster_points.shape[0] > 0:
            new_centers[i] = np.mean(cluster_points, axis=0)
        else:
            new_centers[i] = data[np.random.choice(len(data))]
    return new_centers

def kmeans_clustering(data, k, max_iters=100, tol=1e-4):
    """
    Executes the K-Means++ clustering algorithm.
    Returns: Final centroids and cluster assignments.
    """
    centers = kmeans_plus_plus_init(data, k)
    for _ in range(max_iters):
        cluster_ids = assign_cluster_ids(data, centers)
        new_centers = compute_cluster_representatives(data, cluster_ids, k)
        if np.linalg.norm(new_centers - centers) < tol:
            break
        centers = new_centers
    return centers, cluster_ids

def compute_silhouette_coefficient(data, cluster_ids, k):
    """
    Computes the Silhouette coefficient for each cluster.
    """
    if k == 1:
        return 0  # Not defined for a single cluster

    silhouette_scores = []
    cluster_ids = np.array(cluster_ids)
    for i in range(len(data)):
        same_cluster = data[cluster_ids == cluster_ids[i]]
        other_clusters = [data[cluster_ids == j] for j in range(k) if j != cluster_ids[i] and data[cluster_ids == j].shape[0] > 0]

        if same_cluster.shape[0] > 1:
            a = np.mean([compute_distance(data[i], p) for p in same_cluster if not np.array_equal(p, data[i])])
        else:
            a = 0

        if other_clusters:
            b = np.min([np.mean([compute_distance(data[i], p) for p in cluster]) for cluster in other_clusters])
        else:
            b = 0

        silhouette_scores.append((b - a) / max(a, b) if max(a, b) > 0 else 0)

    return np.mean(silhouette_scores)

def plot_silhouette(data):
    """
    Calculates and plots the Silhouette coefficient for k ranging from 1 to 9.
    """
    k_values = range(1, 10)
    silhouette_scores = []

    for k in k_values:
        _, cluster_ids = kmeans_clustering(data, k)
        silhouette_score = compute_silhouette_coefficient(data, cluster_ids, k)
        silhouette_scores.append(silhouette_score)

    plt.figure(figsize=(8, 5))
    plt.plot(k_values, silhouette_scores, marker='o', linestyle='-')
    plt.xlabel("Number of Clusters (k)")
    plt.ylabel("Silhouette Coefficient")
    plt.title("Silhouette Score vs. Number of Clusters")
    plt.grid()
    plt.savefig("silhouette_plot.png")
    plt.show()

if __name__ == "__main__":
    data = load_dataset()
    plot_silhouette(data)